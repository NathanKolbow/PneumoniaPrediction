{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxDcnEsvcPDz"
   },
   "source": [
    "# Pneumonia\n",
    "\n",
    "Original paper: https://www.cell.com/cell/fulltext/S0092-8674(18)30154-5\n",
    "\n",
    "They achieved 92.8% test accuracy, with 93.2% sensitivity, 90.1% specificity, and area under the ROC curve of 96.8%.\n",
    "\n",
    "For binary comparison of bacterial vs. viral pneumonia they got 90.7% test accuracy with 88.6% sensitivity, 90.9% specificity and area under the ROC curve for distinguishing between the two of 94.0%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_0eFPnxbTZE"
   },
   "source": [
    "## The Code\n",
    "\n",
    "### Imports\n",
    "\n",
    "First we import all of our libraries, mount our Google Drive and define our base directory (these last two are done according to whether or not we are running on a Google runtime or a local Jupyter Notebook runtime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEkDoOOhVkdm",
    "outputId": "43cd82fd-9c73-4c40-cc33-593abd1f746d"
   },
   "outputs": [],
   "source": [
    "import torch  \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "from math import ceil\n",
    "\n",
    "# Uncomment the following if the notebook is being run with Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "# BASE_DIR = '/content/drive/Shareddrives/WetLung'\n",
    "\n",
    "# Uncomment the following if the notebook is being run locally\n",
    "BASE_DIR = '.'\n",
    "\n",
    "\n",
    "IMG_SIZE = (448, 448)\n",
    "COLOR_DEPTH = 24\n",
    "\n",
    "# As calculated in code later on\n",
    "IMG_MEANS = (0.11999158, 0.118331894, 0.11863783)\n",
    "IMG_STDS = (0.17364839, 0.17083007, 0.16970974)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kxBRWj5bbEk"
   },
   "source": [
    "### Quick Sanity Check\n",
    "\n",
    "This small code chunk makes sure that we have hooked up runtime up with our data properly.  If all is well, an image of chest X-ray should be shown after running the code chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Oq6czjoaa3iC",
    "outputId": "9667e7dc-ba4c-4a38-9b53-0125a7a0e2b9"
   },
   "outputs": [],
   "source": [
    "img = Image.open(BASE_DIR + '/chest_xray/train/PNEUMONIA/person530_bacteria_2233.jpeg')\n",
    "\n",
    "jitter = 1.5\n",
    "\n",
    "resz = transforms.Resize(IMG_SIZE)\n",
    "tfrm1 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5)\n",
    "])\n",
    "tfrm2 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(p=1),\n",
    "    transforms.Normalize(0.5, 0.5)\n",
    "])\n",
    "tfrm3 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomRotation(18),\n",
    "    transforms.Normalize(0.5, 0.5)\n",
    "])\n",
    "tfrm4 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5),\n",
    "    transforms.ColorJitter((jitter, jitter), (jitter, jitter), (jitter, jitter))\n",
    "])\n",
    "\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].imshow(torch.tensor((tfrm1(img).permute(1, 2, 0) * 255), dtype=int))\n",
    "axs[0, 0].set_title(\"Original Image\")\n",
    "axs[1, 0].imshow(torch.tensor((tfrm2(img).permute(1, 2, 0) * 255), dtype=int))\n",
    "axs[1, 0].set_title(\"Horizontal Flip\")\n",
    "axs[0, 1].imshow(torch.tensor((tfrm3(img).permute(1, 2, 0) * 255), dtype=int))\n",
    "axs[0, 1].set_title(\"Random Rotation\")\n",
    "axs[1, 1].imshow(torch.tensor((tfrm4(img).permute(1, 2, 0) * 255), dtype=int))\n",
    "axs[1, 1].set_title(\"Color Jitter\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"transforms.png\", dpi=1000)\n",
    "plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(resz(img))\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(tfrm(img).permute(1, 2, 0))\n",
    "\n",
    "# img.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12U50KYbZfJJ"
   },
   "source": [
    "### Data Pre-processing\n",
    "\n",
    "This code chunk is a bit long, but it performs **all** of the data loading, manipulation, and saving that we'll need for the entire project, aside from the random pertubations made to training batches.\n",
    "\n",
    "The code below has a 'safeguard' in place so that it only runs if the data relevant to the global *IMG_SIZE* and *COLOR_DEPTH* does not already exist.  This way we can *Run All Code Chunks* without worrying about running this chunk every time.  The code chunk *will* run and create new, relevant data if we decide to change the *IMG_SIZE* or *COLOR_DEPTH*, though.\n",
    "\n",
    "The entirety of the operations performed by this chunk are the following:\n",
    "\n",
    "1. Walk the */chest_xray/* path and gather all of the image file names.\n",
    "2. All of our files are partitioned once by their membership to the train/test/validation split and then again by whether they are a normal chest X-ray or a pneumonia chest X-ray, so we generate our *y* variables from this.\n",
    "3. One by one, convert each image to *COLOR_DEPTH*-bit color depth, resize the image to *IMG_SIZE*, and compile each of these transformed images into tensors.\n",
    "4. Normalize each of the tensors that were built in step (3).\n",
    "5. Shuffle each of the data splits.  (This part is unnecessary because of our use of PyTorch DataSets and DataLoaders later on, but we do it regardless.\n",
    "6. Save all of the data to files.\n",
    "7. Explicitly free the data as we have it stored now.  This clears up RAM that we will definitely be needing later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8lUxKDIZXVk",
    "outputId": "00408cce-1c7f-45ec-cb1b-39a8c9633bc5"
   },
   "outputs": [],
   "source": [
    "# This function checks whether or not the data pertaining to the defined\n",
    "# IMG_SIZE and COLOR_DEPTH already exists\n",
    "def requisite_data_exists():\n",
    "  for mode in ('train', 'test', 'val'):\n",
    "    if not os.path.exists(BASE_DIR + '/numpy_objects/' + mode + '_X_' + str(IMG_SIZE[1]) + 'x' + str(IMG_SIZE[0]) + '_' + str(COLOR_DEPTH) + 'bit.npy'):\n",
    "      return False\n",
    "    elif not os.path.exists(BASE_DIR + '/torch_objects/' + mode + '_y.pt'):\n",
    "      return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# We *only* want to run this large chunk of data manipulation if the data we're \n",
    "# trying to generate does not already exist\n",
    "if not requisite_data_exists():\n",
    "  # Crude, but loads all of the files into separate arrays\n",
    "  train_files = [glob.glob(BASE_DIR + '/chest_xray/train/' + pathend) for pathend in ['NORMAL/*.jpeg', 'PNEUMONIA/*.jpeg']]\n",
    "  test_files = [glob.glob(BASE_DIR + '/chest_xray/test/' + pathend) for pathend in ['NORMAL/*.jpeg', 'PNEUMONIA/*.jpeg']]\n",
    "  val_files = [glob.glob(BASE_DIR + '/chest_xray/val/' + pathend) for pathend in ['NORMAL/*.jpeg', 'PNEUMONIA/*.jpeg']]\n",
    "\n",
    "  # Create emtpy arrs\n",
    "  train_y = np.empty(len(train_files[0]) + len(train_files[1]))\n",
    "  test_y = np.empty(len(test_files[0]) + len(test_files[1]))\n",
    "  val_y = np.empty(len(val_files[0]) + len(val_files[1]))\n",
    "\n",
    "  # Add the y's for the NORMALs\n",
    "  train_y[0:len(train_files[0])] = 0\n",
    "  test_y[0:len(test_files[0])] = 0\n",
    "  val_y[0:len(val_files[0])] = 0\n",
    "\n",
    "  # Cycle through the pneumonia files and add appropriately\n",
    "  for name, i in zip(train_files[1], range(len(train_files[1]))):\n",
    "    train_y[len(train_files[0]) + i] = 1 if \"bacteria\" in name else 2\n",
    "  for name, i in zip(test_files[1], range(len(test_files[1]))):\n",
    "    test_y[len(test_files[0]) + i] = 1 if \"bacteria\" in name else 2\n",
    "  for name, i in zip(val_files[1], range(len(val_files[1]))):\n",
    "    val_y[len(val_files[0]) + i] = 1 if \"bacteria\" in name else 2\n",
    "\n",
    "  tfrms = transforms.Compose([\n",
    "      transforms.Resize(IMG_SIZE),\n",
    "      transforms.ToTensor()\n",
    "  ])\n",
    "\n",
    "  def transform(img):\n",
    "    # The images vary in color depth so we convert all of them to RGB\n",
    "    # with COLOR_DEPTH-bit color depth\n",
    "    tfrm_img = tfrms(img.convert(\"RGB\", colors=COLOR_DEPTH))\n",
    "    img.close()\n",
    "    return tfrm_img\n",
    "\n",
    "  # We flatten the nested lists of files, load the images from the files, and convert the images to tensors all in one go\n",
    "  print(\"Transforming train_X...\")\n",
    "  train_X = torch.empty((len(train_files[0]) + len(train_files[1]), 3, IMG_SIZE[0], IMG_SIZE[1]), dtype=torch.float32)\n",
    "  for img, i in zip(np.concatenate(train_files), np.arange(len(train_files[0]) + len(train_files[1]))):\n",
    "    train_X[i] = transform(Image.open(img))\n",
    "\n",
    "  print(\"Transforming test_X...\")\n",
    "  test_X = torch.empty((len(test_files[0]) + len(test_files[1]), 3, IMG_SIZE[0], IMG_SIZE[1]), dtype=torch.float32)\n",
    "  for img, i in zip(np.concatenate(test_files), np.arange(len(test_files[0]) + len(test_files[1]))):\n",
    "    test_X[i] = transform(Image.open(img))\n",
    "\n",
    "  print(\"Transforming val_X...\")\n",
    "  val_X = torch.empty((len(val_files[0]) + len(val_files[1]), 3, IMG_SIZE[0], IMG_SIZE[1]), dtype=torch.float32)\n",
    "  for img, i in zip(np.concatenate(val_files), np.arange(len(val_files[0]) + len(val_files[1]))):\n",
    "    val_X[i] = transform(Image.open(img))\n",
    "\n",
    "  # Save our data\n",
    "  print(\"Saving X data...\")\n",
    "  np.save(BASE_DIR + '/numpy_objects/train_X_' + str(IMG_SIZE[1]) + 'x' + str(IMG_SIZE[0]) + '_' + str(COLOR_DEPTH) + 'bit.npy', np.array(train_X), allow_pickle=True)\n",
    "  np.save(BASE_DIR + '/numpy_objects/test_X_' + str(IMG_SIZE[1]) + 'x' + str(IMG_SIZE[0]) + '_' + str(COLOR_DEPTH) + 'bit.npy', np.array(test_X), allow_pickle=True)\n",
    "  np.save(BASE_DIR + '/numpy_objects/val_X_' + str(IMG_SIZE[1]) + 'x' + str(IMG_SIZE[0]) + '_' + str(COLOR_DEPTH) + 'bit.npy', np.array(val_X), allow_pickle=True)\n",
    "\n",
    "  print(\"Saving y data...\")\n",
    "  torch.save(torch.tensor(train_y, dtype=int), BASE_DIR + '/torch_objects/train_y.pt')\n",
    "  torch.save(torch.tensor(test_y, dtype=int), BASE_DIR + '/torch_objects/test_y.pt')\n",
    "  torch.save(torch.tensor(val_y, dtype=int), BASE_DIR + '/torch_objects/val_y.pt')\n",
    "\n",
    "  # Clear out some memory so that we don't overload our RAM\n",
    "  del train_X, test_X, val_X, train_y, test_y, val_y\n",
    "  torch.cuda.empty_cache()\n",
    "else:\n",
    "  print('Requisite data already exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  train_files = [glob.glob(BASE_DIR + '/chest_xray/train/' + pathend) for pathend in ['NORMAL/*.jpeg', 'PNEUMONIA/*.jpeg']]\n",
    "  test_files = [glob.glob(BASE_DIR + '/chest_xray/test/' + pathend) for pathend in ['NORMAL/*.jpeg', 'PNEUMONIA/*.jpeg']]\n",
    "  val_files = [glob.glob(BASE_DIR + '/chest_xray/val/' + pathend) for pathend in ['NORMAL/*.jpeg', 'PNEUMONIA/*.jpeg']]\n",
    "\n",
    "  # Create emtpy arrs\n",
    "  train_y = np.empty(len(train_files[0]) + len(train_files[1]))\n",
    "  test_y = np.empty(len(test_files[0]) + len(test_files[1]))\n",
    "  val_y = np.empty(len(val_files[0]) + len(val_files[1]))\n",
    "\n",
    "  # Add the y's for the NORMALs\n",
    "  train_y[0:len(train_files[0])] = 0\n",
    "  test_y[0:len(test_files[0])] = 0\n",
    "  val_y[0:len(val_files[0])] = 0\n",
    "\n",
    "  # Cycle through the pneumonia files and add appropriately\n",
    "  for name, i in zip(train_files[1], range(len(train_files[1]))):\n",
    "    train_y[len(train_files[0]) + i] = 1 if \"bacteria\" in name else 2\n",
    "  for name, i in zip(test_files[1], range(len(test_files[1]))):\n",
    "    test_y[len(test_files[0]) + i] = 1 if \"bacteria\" in name else 2\n",
    "  for name, i in zip(val_files[1], range(len(val_files[1]))):\n",
    "    val_y[len(val_files[0]) + i] = 1 if \"bacteria\" in name else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(train_y == 0))\n",
    "print(sum(train_y != 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyYkDZgyNqI_"
   },
   "source": [
    "### Mean and Standard Deviation\n",
    "\n",
    "We calculate the mean and variance of our image data so that we can accurately normalize it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsd-d6r4ONpB"
   },
   "outputs": [],
   "source": [
    "# We don't want to run this now because we've already done the calculations\n",
    "if False:\n",
    "  X = np.load(BASE_DIR + '/numpy_objects/' + 'train' + '_X_' + str(IMG_SIZE[1]) + 'x' + str(IMG_SIZE[0]) + '_' + str(COLOR_DEPTH) + 'bit.npy', mmap_mode='r', allow_pickle=True)\n",
    "  c1 = X[:, :, :, 0]\n",
    "  c2 = X[:, :, :, 1]\n",
    "  c3 = X[:, :, :, 2]\n",
    "\n",
    "  img_means = (c1.mean(), c2.mean(), c3.mean())\n",
    "  img_stds  = (c1.std(), c2.std(), c3.std())\n",
    "\n",
    "  print(img_means)\n",
    "  print(img_stds)\n",
    "\n",
    "  del X, c1, c2, c3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qq2He0A88kxK"
   },
   "source": [
    "### Datasets\n",
    "\n",
    "Here, we define an XrayDataset class which extends the PyTorch Dataset module.  Our main motivation for managing our data like this is that we need to load our data in a relatively unique way, and PyTorch Datasets allow us to do this in a very clean manner.\n",
    "\n",
    "Our image data, even downscaled, is too large to fit in RAM all at once when we train our model.  To get around this (to an extent) we load our data into NumPy ndarrays with memory-mapping enabled.  This means that our data is never actually loaded into RAM *until it is accessed*.  This allows us to only load a batch of data into RAM at a time which we can then convert to a PyTorch tensor and give to our model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFXK8AgIkNnV"
   },
   "outputs": [],
   "source": [
    "class XrayDataset(Dataset):\n",
    "  def __init__(self, mode, multi=False):\n",
    "    self.X = np.load(BASE_DIR + '/numpy_objects/' + mode + '_X_' + str(IMG_SIZE[1]) + 'x' + str(IMG_SIZE[0]) + '_' + str(COLOR_DEPTH) + 'bit.npy', mmap_mode='r', allow_pickle=True)\n",
    "    self.y = torch.load(BASE_DIR + '/torch_objects/' + mode + '_y.pt')\n",
    "\n",
    "    if not multi:\n",
    "      self.y[self.y == 2] = 1\n",
    "\n",
    "    self.perspective_skew = 0.1\n",
    "    self.crop_scale = 0.9\n",
    "    self.horiz_flip_proba = 0.25\n",
    "\n",
    "    self.output_size = (int((1 - self.perspective_skew) * self.crop_scale * IMG_SIZE[0]), int((1 - self.perspective_skew) * self.crop_scale * IMG_SIZE[1]))\n",
    "\n",
    "    # If we're providing training data we want FULL data pertubations.\n",
    "    # If we're providing ANY other type of data, we only want to return the data converted to Tensor form.\n",
    "    if mode == 'train':\n",
    "      self.tfrm = transforms.Compose([\n",
    "          transforms.ToTensor(),\n",
    "          transforms.RandomHorizontalFlip(p=self.horiz_flip_proba),\n",
    "          transforms.RandomRotation(10),\n",
    "          transforms.ColorJitter(0.7, 0.7, 0.7),\n",
    "          transforms.Normalize(IMG_MEANS, IMG_STDS)\n",
    "      ])\n",
    "\n",
    "      #################################################################################################################################################\n",
    "      # These transforms were used for all models before (excluding) model 9\n",
    "      #\n",
    "      # self.tfrm = transforms.Compose([\n",
    "      #   # For some reason NumPy float32 arrays aren't handled by transforms.ToPILImage, so we have to convert to Tensor before we convert to PILImage\n",
    "      #   transforms.ToTensor(),\n",
    "      #   transforms.ToPILImage(),\n",
    "\n",
    "      #   # Body orientations can be different during scans, so we don't want the model memorizing body orientations\n",
    "      #   transforms.RandomPerspective(distortion_scale=self.perspective_skew, p=0.75, fill=150),\n",
    "      #   transforms.RandomHorizontalFlip(p=self.horiz_flip_proba),\n",
    "        \n",
    "      #   # The center crop is to remove the majority of the black space left by the RandomPerspective transform\n",
    "      #   transforms.CenterCrop(((1 - self.perspective_skew) * IMG_SIZE[0], (1 - self.perspective_skew) * IMG_SIZE[1])),\n",
    "\n",
    "      #   # Finally we do a random crop so that the model doesn't memorize pixel positions or anything like that\n",
    "      #   # We do 0.8 * IMG_SIZE, but this is really ~0.9 * IMG_SIZE when you take into account the rescaling that\n",
    "      #   # happens naturally from the RandomPerspective transform\n",
    "      #   transforms.RandomCrop(self.output_size),\n",
    "\n",
    "      #   # Re-convert back to Tensor\n",
    "      #   transforms.ToTensor()\n",
    "      # ])\n",
    "      ################################################################################################################################################\n",
    "    else:\n",
    "      # The normalization transform was added here with model 9\n",
    "      self.tfrm = transforms.Compose([\n",
    "          transforms.ToTensor(),\n",
    "          transforms.Normalize(IMG_MEANS, IMG_STDS)\n",
    "      ])\n",
    "\n",
    "  def __len__(self):\n",
    "   return self.y.size(0)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if torch.is_tensor(idx):\n",
    "      idx = idx.tolist()\n",
    "    \n",
    "    if type(idx) == int:\n",
    "      # We're only returning 1 index, so we return the transformed data\n",
    "      ret_X = self.tfrm(self.X[idx].reshape(IMG_SIZE[0], IMG_SIZE[1], COLOR_DEPTH // 8))\n",
    "    else:\n",
    "      # idx is a list in this case, so we transform the NumPy data and then place it in a pre-allocated tensor\n",
    "      ret_X = torch.empty((len(idx), COLOR_DEPTH // 8, self.output_size[0], self.output_size[1]))\n",
    "      for i in range(ret_X.size(0)):\n",
    "        ret_X[i] = self.tfrm(self.X[idx])\n",
    "\n",
    "    return ret_X, self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o872MUBMS20_"
   },
   "source": [
    "### The Neural Model\n",
    "\n",
    "Pretty basic convolutional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IDMq2dF55z6"
   },
   "outputs": [],
   "source": [
    "class YOLOSigmoidModel(torch.nn.Module):\n",
    "  def __init__(self, img_size):\n",
    "    super(YOLOSigmoidModel, self).__init__()\n",
    "\n",
    "    # Required 448x448 image size\n",
    "    if img_size != (448, 448):\n",
    "      raise ValueError(\"Image size must be 448x448 to use the YOLO model.\")\n",
    "    \n",
    "    self.model_ID = 'YOLOSigmoid'\n",
    "\n",
    "    # The network architecture from \"You Only Look Once: Unified, Real-Time Object Detection\"\n",
    "    # https://arxiv.org/abs/1506.02640\n",
    "    self.network = nn.Sequential(\n",
    "        nn.BatchNorm2d(3),\n",
    "        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(4, 4), stride=2),\n",
    "        nn.Sigmoid(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.Conv2d(in_channels=32, out_channels=128, kernel_size=(2, 2), stride=1),\n",
    "        nn.Sigmoid(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        \n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(1, 1), stride=1),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(2, 2), stride=1),\n",
    "        nn.Sigmoid(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(1, 1), stride=1),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(2, 2), stride=1),\n",
    "        nn.Sigmoid(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        \n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(1, 1), stride=1, padding=1),\n",
    "        nn.Sigmoid(),\n",
    "        \n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(2, 2), stride=2, padding=1),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Conv2d(in_channels=512, out_channels=256, kernel_size=(1, 1), stride=1),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(2, 2), stride=2),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Conv2d(in_channels=256, out_channels=128, kernel_size=(1, 1), stride=1),\n",
    "        nn.Sigmoid(),\n",
    "\n",
    "        nn.Flatten(),\n",
    "        nn.BatchNorm1d(2048),\n",
    "        \n",
    "        nn.Linear(2048, 512),\n",
    "        nn.Sigmoid(),\n",
    "\n",
    "        nn.Linear(512, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.network(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10iQQUyBYVH6"
   },
   "outputs": [],
   "source": [
    "class YOLOModel(torch.nn.Module):\n",
    "  def __init__(self, img_size):\n",
    "    super(YOLOModel, self).__init__()\n",
    "\n",
    "    # Required 448x448 image size\n",
    "    if img_size != (448, 448):\n",
    "      raise ValueError(\"Image size must be 448x448 to use the YOLO model.\")\n",
    "    \n",
    "    self.model_ID = 'YOLO'\n",
    "\n",
    "    # The network architecture from \"You Only Look Once: Unified, Real-Time Object Detection\"\n",
    "    # https://arxiv.org/abs/1506.02640\n",
    "    self.network = nn.Sequential(\n",
    "        nn.BatchNorm2d(3),\n",
    "        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(4, 4), stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.Conv2d(in_channels=32, out_channels=128, kernel_size=(2, 2), stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        \n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(1, 1), stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(2, 2), stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(1, 1), stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(2, 2), stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        \n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(1, 1), stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(2, 2), stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512, out_channels=256, kernel_size=(1, 1), stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(2, 2), stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=256, out_channels=128, kernel_size=(1, 1), stride=1),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Flatten(),\n",
    "        nn.BatchNorm1d(2048),\n",
    "        \n",
    "        nn.Linear(2048, 512),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Linear(512, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.network(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-iLkfqAZi_f"
   },
   "outputs": [],
   "source": [
    "class YOLOModelNoBatch(torch.nn.Module):\n",
    "  def __init__(self, img_size):\n",
    "    super(YOLOModelNoBatch, self).__init__()\n",
    "\n",
    "    # Required 448x448 image size\n",
    "    if img_size != (224, 224):\n",
    "      raise ValueError(\"Image size must be 224x224 to use the YOLO model.\")\n",
    "    \n",
    "    self.model_ID = 'YOLO_NoBatch'\n",
    "\n",
    "    # The network architecture from \"You Only Look Once: Unified, Real-Time Object Detection\"\n",
    "    # https://arxiv.org/abs/1506.02640\n",
    "    self.network = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(5, 5), stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "\n",
    "        nn.Conv2d(in_channels=64, out_channels=192, kernel_size=(3, 3), stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        \n",
    "        nn.Conv2d(in_channels=192, out_channels=128, kernel_size=(1, 1), stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(2, 2), stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "\n",
    "        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(1, 1), stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=(3, 3), stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        \n",
    "        nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(1, 1), stride=1),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Flatten(),\n",
    "\n",
    "        nn.Linear(9216, 4096),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Linear(4096, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.network(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-XizsILRjXk"
   },
   "outputs": [],
   "source": [
    "class CNNModel1(torch.nn.Module):\n",
    "  def __init__(self, img_size):\n",
    "    super(CNNModel1, self).__init__()\n",
    "    self.drop_proba = 0.5\n",
    "    self.model_ID = '1'\n",
    "\n",
    "    self.network = nn.Sequential(\n",
    "        # Fully convolutional (not this network right now) reference paper: https://arxiv.org/pdf/1411.4038.pdf\n",
    "        nn.Conv2d(3, 6, 8),\n",
    "        nn.Conv2d(6, 12, 16),\n",
    "        nn.MaxPool2d(4, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(12),\n",
    "        # nn.Dropout(self.drop_proba), try fitting model w/o dropout first, only add dropout once overfitting occurs\n",
    "        nn.Conv2d(12, 18, 24),\n",
    "        nn.MaxPool2d(3, 2),\n",
    "        nn.BatchNorm2d(18),\n",
    "        nn.ReLU(),\n",
    "        # nn.Dropout(self.drop_proba),\n",
    "        nn.Conv2d(18, 36, 24),\n",
    "        nn.MaxPool2d(3, 2),\n",
    "        nn.BatchNorm2d(36),\n",
    "        nn.ReLU(),\n",
    "        # nn.Dropout(self.drop_proba),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(17892, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.Linear(1024, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.Linear(128, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.network(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBYKh5gOHt7p"
   },
   "outputs": [],
   "source": [
    "class CNNModel2(torch.nn.Module):\n",
    "  def __init__(self, img_size):\n",
    "    super(CNNModel2, self).__init__()\n",
    "    self.drop_proba = 0.4\n",
    "    self.model_ID = '2'\n",
    "\n",
    "    self.network = nn.Sequential(\n",
    "        # Fully convolutional (not this network right now) reference paper: https://arxiv.org/pdf/1411.4038.pdf\n",
    "        nn.Conv2d(3, 6, 8),\n",
    "        nn.Conv2d(6, 12, 16),\n",
    "        nn.MaxPool2d(4, 1),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(12),\n",
    "        nn.Dropout(self.drop_proba), # try fitting model w/o dropout first, only add dropout once overfitting occurs\n",
    "        nn.Conv2d(12, 18, 24),\n",
    "        nn.MaxPool2d(3, 2),\n",
    "        nn.BatchNorm2d(18),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(18, 36, 24),\n",
    "        nn.MaxPool2d(3, 2),\n",
    "        nn.BatchNorm2d(36),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(17892, 1024),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.Linear(1024, 128),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.Linear(128, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.network(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Qi9fe9l2orS"
   },
   "outputs": [],
   "source": [
    "class CNNModel3(torch.nn.Module):\n",
    "  def __init__(self, img_size):\n",
    "    super(CNNModel3, self).__init__()\n",
    "    self.drop_proba = 0.9\n",
    "    self.model_ID = '3'\n",
    "\n",
    "    self.network = nn.Sequential(\n",
    "        # Fully convolutional (not this network right now) reference paper: https://arxiv.org/pdf/1411.4038.pdf\n",
    "        nn.Conv2d(3, 6, 8),\n",
    "        nn.Conv2d(6, 12, 16),\n",
    "        nn.MaxPool2d(4, 1),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(12),\n",
    "        nn.Dropout(self.drop_proba), # try fitting model w/o dropout first, only add dropout once overfitting occurs\n",
    "        nn.Conv2d(12, 18, 24),\n",
    "        nn.MaxPool2d(3, 2),\n",
    "        nn.BatchNorm2d(18),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(18, 36, 24),\n",
    "        nn.MaxPool2d(3, 2),\n",
    "        nn.BatchNorm2d(36),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(17892, 1024),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.Linear(1024, 128),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(self.drop_proba),\n",
    "        nn.Linear(128, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.network(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZ_t2T-gxm2J"
   },
   "outputs": [],
   "source": [
    "class CNNModel4(torch.nn.Module):\n",
    "  def __init__(self, img_size):\n",
    "    super(CNNModel4, self).__init__()\n",
    "    self.convolutional_drop_proba = 0.2\n",
    "    self.linear_drop_proba = 0.2\n",
    "    self.model_ID = '4'\n",
    "\n",
    "    self.network = nn.Sequential(\n",
    "        # ~very vaguely~ modeled off of the ResNet50 architecture, just without the Res part\n",
    "        nn.Conv2d(3, 64, 7, stride=2),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(64),\n",
    "\n",
    "        nn.Conv2d(64, 64, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 256, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        nn.Dropout(self.convolutional_drop_proba),\n",
    "\n",
    "        nn.Conv2d(256, 256, 3),\n",
    "        nn.MaxPool2d(3, 2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Dropout(self.convolutional_drop_proba),\n",
    "\n",
    "        nn.AvgPool2d(3, padding=1),\n",
    "        nn.Flatten(),\n",
    "\n",
    "        nn.Linear(3584, 1024),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Linear(1024, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.network(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04O0Srf66Mm0"
   },
   "outputs": [],
   "source": [
    "class CNNModel5(torch.nn.Module):\n",
    "  def __init__(self, img_size):\n",
    "    super(CNNModel5, self).__init__()\n",
    "    self.convolutional_drop_proba = 0.35\n",
    "    self.linear_drop_proba = 0.35\n",
    "    self.model_ID = '5'\n",
    "\n",
    "    self.network = nn.Sequential(\n",
    "        # ~very vaguely~ modeled off of the ResNet50 architecture, just without the Res part\n",
    "        nn.Conv2d(3, 64, 7, stride=2),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(64),\n",
    "\n",
    "        nn.Conv2d(64, 64, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 256, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        nn.Dropout(self.convolutional_drop_proba),\n",
    "\n",
    "        nn.Conv2d(256, 256, 3),\n",
    "        nn.MaxPool2d(3, 2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Dropout(self.convolutional_drop_proba),\n",
    "\n",
    "        nn.AvgPool2d(3, padding=1),\n",
    "        nn.Flatten(),\n",
    "\n",
    "        nn.Linear(3584, 1024),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Linear(1024, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.network(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBALQzvM51eA"
   },
   "outputs": [],
   "source": [
    "class CNNModel6(torch.nn.Module):\n",
    "  def __init__(self, img_size):\n",
    "    super(CNNModel6, self).__init__()\n",
    "    self.convolutional_drop_proba = 0.6\n",
    "    self.linear_drop_proba = 0.35\n",
    "    self.model_ID = '6'\n",
    "\n",
    "    self.network = nn.Sequential(\n",
    "        # ~very vaguely~ modeled off of the ResNet50 architecture, just without the Res part\n",
    "        nn.Conv2d(3, 64, 7, stride=2),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(64),\n",
    "\n",
    "        nn.Conv2d(64, 64, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 256, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        nn.Dropout(self.convolutional_drop_proba),\n",
    "\n",
    "        nn.Conv2d(256, 256, 3),\n",
    "        nn.MaxPool2d(3, 2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Dropout(self.convolutional_drop_proba),\n",
    "\n",
    "        nn.AvgPool2d(3, padding=1),\n",
    "        nn.Flatten(),\n",
    "\n",
    "        nn.Linear(3584, 1024),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Linear(1024, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.network(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5-bR4BcpDrm"
   },
   "outputs": [],
   "source": [
    "class CNNModel7(torch.nn.Module):\n",
    "  def __init__(self, img_size):\n",
    "    super(CNNModel7, self).__init__()\n",
    "    self.convolutional_drop_proba = 0.4\n",
    "    self.linear_drop_proba = 0.25\n",
    "    self.model_ID = '7'\n",
    "\n",
    "    self.network = nn.Sequential(\n",
    "        # ~very vaguely~ modeled off of the ResNet50 architecture, just without the Res part\n",
    "        nn.Conv2d(3, 64, 7, stride=2),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.Dropout(self.convolutional_drop_proba),\n",
    "\n",
    "        nn.Conv2d(64, 64, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 256, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        nn.Dropout(self.convolutional_drop_proba),\n",
    "\n",
    "        nn.Conv2d(256, 256, 3),\n",
    "        nn.MaxPool2d(3, 2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Dropout(self.convolutional_drop_proba),\n",
    "\n",
    "        nn.AvgPool2d(3, padding=1),\n",
    "        nn.Flatten(),\n",
    "\n",
    "        nn.Linear(3584, 1024),\n",
    "        nn.Dropout(self.linear_drop_proba),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Linear(1024, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.network(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6KUXMa-UVEJ"
   },
   "outputs": [],
   "source": [
    "class CNNModel8(torch.nn.Module):\n",
    "  def __init__(self, img_size):\n",
    "    super(CNNModel8, self).__init__()\n",
    "    self.convolutional_drop_proba = 0.6\n",
    "    self.linear_drop_proba = 0.5\n",
    "    self.model_ID = '8'\n",
    "\n",
    "    self.network = nn.Sequential(\n",
    "        # ~very vaguely~ modeled off of the ResNet50 architecture, just without the Res part\n",
    "        nn.Conv2d(3, 64, 7, stride=2),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.Dropout(self.convolutional_drop_proba),\n",
    "\n",
    "        nn.Conv2d(64, 64, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 256, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        nn.Dropout(self.convolutional_drop_proba),\n",
    "\n",
    "        nn.Conv2d(256, 256, 3),\n",
    "        nn.MaxPool2d(3, 2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Dropout(self.convolutional_drop_proba),\n",
    "\n",
    "        nn.AvgPool2d(3, padding=1),\n",
    "        nn.Flatten(),\n",
    "\n",
    "        nn.Linear(1536, 512),\n",
    "        nn.Dropout(self.linear_drop_proba),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Linear(512, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.network(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYc3BNeulf-v"
   },
   "outputs": [],
   "source": [
    "class CNNModel9(torch.nn.Module):\n",
    "  def __init__(self, img_size):\n",
    "    super(CNNModel9, self).__init__()\n",
    "    self.convolutional_drop_proba = 0.6\n",
    "    self.linear_drop_proba = 0.5\n",
    "    self.model_ID = '9'\n",
    "\n",
    "    self.network = nn.Sequential(\n",
    "        # ~very vaguely~ modeled off of the ResNet50 architecture, just without the Res part\n",
    "        nn.Conv2d(3, 64, 7, stride=2),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.Dropout2d(self.convolutional_drop_proba),\n",
    "\n",
    "        nn.Conv2d(64, 64, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 256, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        nn.Dropout2d(self.convolutional_drop_proba),\n",
    "\n",
    "        nn.Conv2d(256, 256, 3),\n",
    "        nn.MaxPool2d(3, 2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Dropout(self.convolutional_drop_proba),\n",
    "\n",
    "        nn.AvgPool2d(3, padding=1),\n",
    "        nn.Flatten(),\n",
    "\n",
    "        nn.Linear(3584, 512),\n",
    "        nn.Dropout(self.linear_drop_proba),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Linear(512, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.network(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVW1qzZHYVyg"
   },
   "source": [
    "### Model Training\n",
    "\n",
    "Here we define a method to train a given model.  The main features to take note of here are the use of DataLoaders both for training and evaluation and the model checkpointing.\n",
    "\n",
    "The more important of the two, the model checkpointing let's us save and run our models progressively in case our runtimes ever crash, or we end up having to shut down our computers early.  In this vain, we can also continue to track training and evaluation loss and accuracy by providing the saved arrays as arguments to the function.\n",
    "\n",
    "In checkpointing, we only record the total number of epochs that we have gone through, but we still checkpoint the model at the end of each batch for an increased progress saving granularity.  This is especially important because single batches can take 10 minutes, which is progress we would rather not lose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAVRIr6A7AH1"
   },
   "outputs": [],
   "source": [
    "def train_binary_model(model, resume_training, num_epochs, train_dataloader, eval_dataloader, test_dataloader, optimizer, model_ID, resume_from=-1):\n",
    "  # Set initial metric/save values\n",
    "  train_cost=[]\n",
    "  eval_cost=[]\n",
    "  train_acc=[]\n",
    "  eval_acc=[]\n",
    "  test_cost=[]\n",
    "  test_acc=[]\n",
    "  model_dicts=[]\n",
    "  opti_dicts=[]\n",
    "  chkpt_epochs=0\n",
    "  \n",
    "  if resume_training:\n",
    "    # Check if there is an applicable previous save\n",
    "    if os.path.exists(BASE_DIR + '/torch_objects/model' +  model.model_ID + '.pt'):\n",
    "      # Load the checkpoint\n",
    "      checkpoint = torch.load(BASE_DIR + '/torch_objects/model' + model.model_ID + '.pt')\n",
    "\n",
    "      # Reload the model/optimizer states\n",
    "      model.load_state_dict(checkpoint['model_state_dicts'][resume_from])\n",
    "      optimizer.load_state_dict(checkpoint['optimizer_state_dicts'][resume_from])\n",
    "\n",
    "      # Reload everything else\n",
    "      last = resume_from+1 if resume_from != 1 else resume_from\n",
    "      train_cost = checkpoint['train_cost'][:last]\n",
    "      eval_cost = checkpoint['eval_cost'][:last]\n",
    "      train_acc = checkpoint['train_acc'][:last]\n",
    "      eval_acc = checkpoint['eval_acc'][:last]\n",
    "      test_cost = checkpoint['test_cost'][:last]\n",
    "      test_acc = checkpoint['test_acc'][:last]\n",
    "      test_cost = checkpoint['test_cost'][:last]\n",
    "      model_dicts = checkpoint['model_state_dicts'][:last]\n",
    "      opti_dicts = checkpoint['optimizer_state_dicts'][:last]\n",
    "      chkpt_epochs = checkpoint['epoch'] if resume_from == -1 else resume_from\n",
    "\n",
    "  for epoch in range(ceil(num_epochs - chkpt_epochs)):\n",
    "    model.train()\n",
    "    for i_batch, (batch_X, batch_y) in enumerate(train_dataloader):\n",
    "      # Adjust batch_y to be binary\n",
    "      batch_y[batch_y == 2] = 1\n",
    "      batch_y = batch_y.view(-1, 1)\n",
    "\n",
    "      # Forward pass\n",
    "      logits = model.forward(batch_X)\n",
    "      loss = F.binary_cross_entropy_with_logits(logits, batch_y.double())\n",
    "\n",
    "      # Zero the gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Backwards propogation\n",
    "      loss.backward()\n",
    "\n",
    "      # Print partial progress\n",
    "      if not i_batch % 5:\n",
    "        print('Epoch: %d/%d | Batch: %d/%d | Loss: %.3f' % (epoch+chkpt_epochs+1, num_epochs, i_batch+1, len(train_dataloader), loss.item()))\n",
    "\n",
    "      # Update parameters\n",
    "      optimizer.step()\n",
    "      \n",
    "    # Gather evaluation metrics\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      # Record the training cost and accuracy\n",
    "      temp_cost = temp_acc = 0\n",
    "      for i_batch, (batch_X, batch_y) in enumerate(train_dataloader):\n",
    "        # Adjust batch_y to be binary\n",
    "        batch_y[batch_y == 2] = 1\n",
    "        batch_y = batch_y.view(-1, 1)\n",
    "\n",
    "        yhat = model.forward(batch_X)\n",
    "        temp_cost += F.binary_cross_entropy_with_logits(yhat, batch_y.double()).item()\n",
    "        temp_acc += torch.sum(torch.round(torch.sigmoid(yhat)) == batch_y).item()\n",
    "      train_cost.append(temp_cost)\n",
    "      train_acc.append(temp_acc / len(train_dataloader.dataset))\n",
    "      print('Training accuracy: %.3f%%' % (train_acc[-1]*100))\n",
    "      \n",
    "      # Record the evaluation cost and accuracy\n",
    "      temp_cost = temp_acc = 0\n",
    "      for i_batch, (batch_X, batch_y) in enumerate(val_dataloader):\n",
    "        # Adjust batch_y to be binary\n",
    "        batch_y[batch_y == 2] = 1\n",
    "        batch_y = batch_y.view(-1, 1)\n",
    "\n",
    "        yhat = model.forward(batch_X)\n",
    "        temp_cost += F.binary_cross_entropy_with_logits(yhat, batch_y.double()).item()\n",
    "        temp_acc += torch.sum(torch.round(torch.sigmoid(yhat)) == batch_y).item()\n",
    "      eval_cost.append(temp_cost)\n",
    "      eval_acc.append(temp_acc / len(eval_dataloader.dataset))\n",
    "      print('Validation accuracy: %.3f%%' % (eval_acc[-1]*100))\n",
    "\n",
    "      # Record the test cost and accuracy\n",
    "      temp_cost = temp_acc = 0\n",
    "      for i_batch, (batch_X, batch_y) in enumerate(test_dataloader):\n",
    "        # Adjust batch_y to be binary\n",
    "        batch_y[batch_y == 2] = 1\n",
    "        batch_y = batch_y.view(-1, 1)\n",
    "\n",
    "        yhat = model.forward(batch_X)\n",
    "        temp_cost += F.binary_cross_entropy_with_logits(yhat, batch_y.double()).item()\n",
    "        temp_acc += torch.sum(torch.round(torch.sigmoid(yhat)) == batch_y).item()\n",
    "      test_cost.append(temp_cost)\n",
    "      test_acc.append(temp_acc / len(test_dataloader.dataset))\n",
    "      print('Test accuracy: %.3f%%' % (test_acc[-1]*100))\n",
    "    model.train()\n",
    "\n",
    "    # Checkpoint the model\n",
    "    model_dicts.append(model.state_dict().copy())\n",
    "    opti_dicts.append(optimizer.state_dict().copy())\n",
    "    torch.save({\n",
    "        'model_state_dicts': model_dicts,\n",
    "        'optimizer_state_dicts': opti_dicts,\n",
    "        'epoch': epoch+chkpt_epochs+1,\n",
    "        'train_cost': train_cost,\n",
    "        'eval_cost': eval_cost,\n",
    "        'train_acc': train_acc,\n",
    "        'eval_acc': eval_acc,\n",
    "        'test_cost': test_cost,\n",
    "        'test_acc': test_acc\n",
    "    }, BASE_DIR + '/torch_objects/model' + str(model_ID) + '.pt')\n",
    "\n",
    "  return train_cost, eval_cost, train_acc, eval_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fS7BXTCtEIOO"
   },
   "outputs": [],
   "source": [
    "def train_multiclass_model(model, resume_training, num_epochs, train_dataloader, eval_dataloader, test_dataloader, optimizer):\n",
    "  # Set initial metric/save values\n",
    "  train_cost=eval_cost=train_acc=eval_acc=test_cost=test_acc=model_dicts=opti_dicts=[]\n",
    "  chkpt_epochs=0\n",
    "  \n",
    "  if resume_training:\n",
    "    # Check if there is an applicable previous save\n",
    "    if os.path.exists(BASE_DIR + '/torch_objects/model' +  model.model_ID + '.pt'):\n",
    "      # Load the checkpoint\n",
    "      checkpoint = torch.load(BASE_DIR + '/torch_objects/model' + model.model_ID + '.pt')\n",
    "\n",
    "      # Reload the model/optimizer states\n",
    "      model.load_state_dict(checkpoint['model_state_dicts'][-1])\n",
    "      optimizer.load_state_dict(checkpoint['optimizer_state_dicts'][-1])\n",
    "\n",
    "      # Reload everything else\n",
    "      train_cost = checkpoint['train_cost']\n",
    "      eval_cost = checkpoint['eval_cost']\n",
    "      train_acc = checkpoint['train_acc']\n",
    "      eval_acc = checkpoint['eval_acc']\n",
    "      test_cost = checkpoint['test_cost']\n",
    "      test_acc = checkpoint['test_acc']\n",
    "      model_dicts = checkpoint['model_state_dicts']\n",
    "      opti_dicts = checkpoint['optimizer_state_dicts']\n",
    "      chkpt_epochs = checkpoint['epoch']\n",
    "\n",
    "  for epoch in range(ceil(num_epochs - chkpt_epochs)):\n",
    "    model.train()\n",
    "    for i_batch, (batch_X, batch_y) in enumerate(train_dataloader):\n",
    "      # TODO: Make some random pertubations to the data before we train on it\n",
    "\n",
    "      # Forward pass\n",
    "      logits = model.forward(batch_X)\n",
    "      loss = F.cross_entropy(logits, batch_y)\n",
    "\n",
    "      # Zero the gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Backwards propogation\n",
    "      loss.backward()\n",
    "\n",
    "      # Print partial progress\n",
    "      if not i_batch % 5:\n",
    "        print('Epoch: %d/%d | Batch: %d/%d | Loss: %.3f' % (epoch+chkpt_epochs+1, num_epochs, i_batch+1, len(train_dataloader), loss.item()))\n",
    "\n",
    "      # Update parameters\n",
    "      optimizer.step()\n",
    "      \n",
    "    # Gather evaluation metrics\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      # Record the training cost and accuracy\n",
    "      temp_cost = temp_acc = 0\n",
    "      for i_batch, (batch_X, batch_y) in enumerate(train_dataloader):\n",
    "        yhat = model.forward(batch_X)\n",
    "        temp_cost += F.cross_entropy(yhat, batch_y).item()\n",
    "        temp_acc += torch.sum(torch.argmax(yhat, dim=1) == batch_y).item()\n",
    "      train_cost.append(temp_cost)\n",
    "      train_acc.append(temp_acc / len(train_dataloader.dataset))\n",
    "      print('Training accuracy: %.3f%%' % (train_acc[-1]*100))\n",
    "      \n",
    "      # Record the evaluation cost and accuracy\n",
    "      temp_cost = temp_acc = 0\n",
    "      for i_batch, (batch_X, batch_y) in enumerate(val_dataloader):\n",
    "        yhat = model.forward(batch_X)\n",
    "        temp_cost += F.cross_entropy(yhat, batch_y).item()\n",
    "        temp_acc += torch.sum(torch.argmax(yhat, dim=1) == batch_y).item()\n",
    "      eval_cost.append(temp_cost)\n",
    "      eval_acc.append(temp_acc / len(eval_dataloader.dataset))\n",
    "      print('Validation accuracy: %.3f%%' % (eval_acc[-1]*100))\n",
    "\n",
    "      # Record the test cost and accuracy\n",
    "      temp_cost = temp_acc = 0\n",
    "      for i_batch, (batch_X, batch_y) in enumerate(test_dataloader):\n",
    "        yhat = model.forward(batch_X)\n",
    "        temp_cost += F.cross_entropy(yhat, batch_y).item()\n",
    "        temp_acc += torch.sum(torch.argmax(yhat, dim=1) == batch_y).item()\n",
    "      test_cost.append(temp_cost)\n",
    "      test_acc.append(temp_acc / len(test_dataloader.dataset))\n",
    "      print('Test accuracy: %.3f%%' % (test_acc[-1]*100))\n",
    "\n",
    "    # Checkpoint the model\n",
    "    model_dicts.append(model.state_dict().copy())\n",
    "    opti_dicts.append(optimizer.state_dict().copy())\n",
    "    torch.save({\n",
    "        'model_state_dicts': model_dicts,\n",
    "        'optimizer_state_dicts': opti_dicts,\n",
    "        'epoch': epoch+chkpt_epochs+1,\n",
    "        'train_cost': train_cost,\n",
    "        'eval_cost': eval_cost,\n",
    "        'train_acc': train_acc,\n",
    "        'eval_acc': eval_acc,\n",
    "        'test_cost': test_cost,\n",
    "        'test_acc': test_acc\n",
    "    }, BASE_DIR + '/torch_objects/model' + model.model_ID + '.pt')\n",
    "\n",
    "    model.train()\n",
    "\n",
    "  return train_cost, eval_cost, test_cost, train_acc, eval_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GxCMkyp_JDu"
   },
   "source": [
    "Finally, all we have to do is set up our model and datasets and start training.\n",
    "\n",
    "We also use this block of code to store notes regarding each of the models we've trained, how they performed, what we changed as we went along, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpmRNHH4_IQU",
    "outputId": "4c00e5d3-d230-4516-848f-2859450eb833"
   },
   "outputs": [],
   "source": [
    "# Build the datasets\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = XrayDataset('train')\n",
    "val_dataset = XrayDataset('val')\n",
    "test_dataset = XrayDataset('test')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "# Train the models\n",
    " \n",
    "# Model 1\n",
    "# Note: starts massively overfitting after epoch 8, after epoch 7 though it had\n",
    "# 89.8% training acc and 87.5% validation acc\n",
    "# Could just be worth focusing on optimizing this model\n",
    "#\n",
    "# model = CNNModel1(IMG_SIZE)\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Model 2\n",
    "# somehow started overfitting faster than model 1, had 94% training acc and <60% val acc after epoch 2\n",
    "#\n",
    "# model = CNNModel2(IMG_SIZE)\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Model 3\n",
    "# This model got stuck @ 74% training accuracy and 50% eval accuracy,\n",
    "# # Update to this note: turns out it's normal for the models to get stuck @ 74%/50%, so it may be worth running this model again\n",
    "#\n",
    "# model = CNNModel3(IMG_SIZE)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.99)\n",
    "\n",
    "# Model 4\n",
    "# First run through had 100% val accuracy on epoch 10, but we don't save ALL state dicts so we didn't get to keep that model\n",
    "# Running again to try to reach that level of validation accuracy again\n",
    "# #\n",
    "# model = CNNModel4(IMG_SIZE) \n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Model 5\n",
    "# Model 4 had super good performance one time at epoch 10 as noted, but in later runs it started overfitting pretty quickly, so model 5\n",
    "# is model 4 but with higher dropout probabilities\n",
    "#\n",
    "# Best validation acc was 87.5%, almost got 100% on training, i.e. overfit\n",
    "#\n",
    "# model = CNNModel5(IMG_SIZE)\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Model 6\n",
    "# Model 5 but w/ even higher dropout probs\n",
    "#\n",
    "# Still overfitting pretty quickly\n",
    "#\n",
    "# model = CNNModel6(IMG_SIZE)\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Model 7\n",
    "# This time we re-add a dropout layer to the fully connected portion of the architecture and increase the drop proba for the convolutional dropout layers\n",
    "#\n",
    "# model = CNNModel7(IMG_SIZE)\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Model 8\n",
    "# Model 7 was STILL overfitting so we're pumping the dropout probabilities wayyyyyyyyy up to see what happens\n",
    "# If this overfits, we should try separately having only the linear and only the convolutional dropout probas high while the other is low\n",
    "#\n",
    "# One of the runs for this model had eval accuracy of 100%, so should check the test accuracy on that epoch's model\n",
    "#\n",
    "# model = CNNModel8(IMG_SIZE)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "################################################################################################################\n",
    "# NOTE: ALL training done before this point was done WITHOUT applying random pertubations to the training data #\n",
    "################################################################################################################\n",
    "\n",
    "# Model 8, now with training sample pertubations\n",
    "# model = CNNModel8(IMG_SIZE)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Model 9, changed Dropout layers that were next to Conv2d layers to Dropout2d\n",
    "# model = CNNModel9(IMG_SIZE)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "################################################################################################################\n",
    "# NOTE: ALL training done before this point was done wit the ORIGINAL train/test/val splits. Past this point,  #\n",
    "#       some data was moved from test to val so that validation accuracies could be meaningfully interpretted. #\n",
    "################################################################################################################\n",
    "\n",
    "# YOLO model w/o batch norm, i.e. w/ ReLU, only running b/c the professor was curious about performance once activations are added\n",
    "# model = YOLOModelNoBatch(IMG_SIZE)\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# YOLO model\n",
    "# Funny enough, the first time I ran this model I totally forgot to put any activations in the convlutional layers\n",
    "# and didn't put BatchNorms anywhere, but the model still got up to 99% training accuracy and >90% validation accuracy.\n",
    "# These models were lost in an accidental file overwrite, but this is interesting to note.\n",
    "# The model now includes ReLU activations and BatchNorms\n",
    "#\n",
    "# This model uses the newly calculated means/std's of the image data instead of 0.5 and 0.5 like other models used\n",
    "#\n",
    "# Ran into massive, fast overfitting issues with 224x224 images, but I think I remember 448x448 images not having as big of\n",
    "# a problem with overfitting, so going back to that resolution\n",
    "#\n",
    "# Getting ~85% test acc on 448x448 is trivial, getting past that is very difficult though\n",
    "# model = YOLOModel(IMG_SIZE)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "\n",
    "model = YOLOSigmoidModel(IMG_SIZE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 100 epochs takes an enormous amount of time to get through, but we have checkpointing enabled, so who cares!\n",
    "train_cost, eval_cost, train_acc, eval_acc = train_binary_model(model, False, 100, train_dataloader, val_dataloader, test_dataloader, optimizer, model.model_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJznPPqwRTeK"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# THIS CODE BLOCK IS SPECIFICALLY FOR GENERATING THE YOLOMODEL ACC/LOSS PLOT #\n",
    "##############################################################################\n",
    "\n",
    "# Testing the performance of the the best performing iteration of model 7\n",
    "model = YOLOModel(IMG_SIZE)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "checkpoint = torch.load(BASE_DIR + '/torch_objects/modelYOLO_first100.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = XrayDataset('train')\n",
    "val_dataset = XrayDataset('val')\n",
    "test_dataset = XrayDataset('test')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(checkpoint['train_acc'])), checkpoint['train_acc'], label=\"Training\")\n",
    "plt.plot(range(len(checkpoint['eval_acc'])), checkpoint['eval_acc'], label=\"Validation\")\n",
    "plt.plot(range(len(checkpoint['test_acc'])), checkpoint['test_acc'], label=\"Testing\")\n",
    "\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"YOLOModel_first100_acc.png\", dpi=1000)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "adj_costs = torch.stack([torch.tensor(checkpoint['train_cost']) / len(train_dataloader), torch.tensor(checkpoint['eval_cost']) / len(val_dataloader), torch.tensor(checkpoint['test_cost']) / len(test_dataloader)])\n",
    "adj_costs[range(0, 3, 2), 14:] /= 2\n",
    "length = len(checkpoint['train_cost'])\n",
    "\n",
    "plt.plot(range(length), adj_costs[0], label=\"Training\")\n",
    "plt.plot(range(length), adj_costs[1], label=\"Validation\")\n",
    "plt.plot(range(length), adj_costs[2], label=\"Testing\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"YOLOModel_first100_loss.png\", dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# THIS CODE BLOCK IS SPECIFICALLY FOR GENERATING THE YOLOMODEL ACC/LOSS PLOT #\n",
    "##############################################################################\n",
    "\n",
    "# Testing the performance of the the best performing iteration of model 7\n",
    "model = YOLOModel(IMG_SIZE)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "checkpoint = torch.load(BASE_DIR + '/torch_objects/modelYOLO-allReLU.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = XrayDataset('train')\n",
    "val_dataset = XrayDataset('val')\n",
    "test_dataset = XrayDataset('test')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(checkpoint['train_acc'])), checkpoint['train_acc'], label=\"Training\")\n",
    "plt.plot(range(len(checkpoint['eval_acc'])), checkpoint['eval_acc'], label=\"Validation\")\n",
    "plt.plot(range(len(checkpoint['test_acc'])), checkpoint['test_acc'], label=\"Testing\")\n",
    "\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"YOLOModel_allReLU_acc.png\", dpi=1000)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "adj_costs = torch.stack([torch.tensor(checkpoint['train_cost']) / len(train_dataloader), torch.tensor(checkpoint['eval_cost']) / len(val_dataloader), torch.tensor(checkpoint['test_cost']) / len(test_dataloader)])\n",
    "# adj_costs[range(0, 3, 2), 17:] /= 2\n",
    "length = len(checkpoint['train_cost'])\n",
    "\n",
    "plt.plot(range(length), adj_costs[0], label=\"Training\")\n",
    "plt.plot(range(length), adj_costs[1], label=\"Validation\")\n",
    "plt.plot(range(length), adj_costs[2], label=\"Testing\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"YOLOModel_allReLU_loss.png\", dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# THIS CODE BLOCK IS SPECIFICALLY FOR GENERATING THE YOLOMODEL ACC/LOSS PLOT #\n",
    "##############################################################################\n",
    "\n",
    "# Testing the performance of the the best performing iteration of model 7\n",
    "model = YOLOSigmoidModel(IMG_SIZE)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "checkpoint = torch.load(BASE_DIR + '/torch_objects/modelYOLOSigmoid.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = XrayDataset('train')\n",
    "val_dataset = XrayDataset('val')\n",
    "test_dataset = XrayDataset('test')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(checkpoint['train_acc'])), checkpoint['train_acc'], label=\"Training\")\n",
    "plt.plot(range(len(checkpoint['eval_acc'])), checkpoint['eval_acc'], label=\"Validation\")\n",
    "plt.plot(range(len(checkpoint['test_acc'])), checkpoint['test_acc'], label=\"Testing\")\n",
    "\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"YOLOModel_Sigmoid_acc.png\", dpi=1000)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "adj_costs = torch.stack([torch.tensor(checkpoint['train_cost']) / len(train_dataloader), torch.tensor(checkpoint['eval_cost']) / len(val_dataloader), torch.tensor(checkpoint['test_cost']) / len(test_dataloader)])\n",
    "# adj_costs[range(0, 3, 2), 17:] /= 2\n",
    "length = len(checkpoint['train_cost'])\n",
    "\n",
    "plt.plot(range(length), adj_costs[0], label=\"Training\")\n",
    "plt.plot(range(length), adj_costs[1], label=\"Validation\")\n",
    "plt.plot(range(length), adj_costs[2], label=\"Testing\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"YOLOModel_Sigmoid_loss.png\", dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-OKyU70TS1C"
   },
   "outputs": [],
   "source": [
    "# Sens/spec of the all ReLU model\n",
    "\n",
    "# Sensitivity: true positives / total positives\n",
    "# Specificity: true negatives / total negatives\n",
    "\n",
    "IMG_SIZE = (448, 448)\n",
    "\n",
    "test_dataset = XrayDataset('test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "model = YOLOModel(IMG_SIZE)\n",
    "checkpoint = torch.load(BASE_DIR + '/torch_objects/modelYOLO-allReLU.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dicts'][76])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "true_neg = 0\n",
    "true_pos = 0\n",
    "total_pos = 0\n",
    "total_neg = 0\n",
    "for i_batch, (batch_X, batch_y) in enumerate(test_dataloader):\n",
    "  yhat = torch.round(torch.sigmoid(model.forward(batch_X)))\n",
    "  total += yhat.size(0)\n",
    "\n",
    "  batch_y = torch.flatten(batch_y)\n",
    "  y_compare = torch.flatten(yhat) == batch_y\n",
    "\n",
    "  # Very inefficient calculation but we don't really care about that b/c these\n",
    "  # calculations are incredibly fast compared to the speed of model.forward(...)\n",
    "  correct += torch.sum(y_compare)\n",
    "  true_pos += torch.sum(torch.logical_and(y_compare, batch_y == 1))\n",
    "  total_pos += torch.sum(batch_y == 1)\n",
    "  true_neg += torch.sum(torch.logical_and(y_compare, batch_y == 0))\n",
    "  total_neg += torch.sum(batch_y == 0)\n",
    "\n",
    "print('\\nOverall test accuracy for model[%d]: %.3f%%' % (26, correct / total * 100))\n",
    "print('Sensitivity: %.3f%%' % (true_pos / total_pos * 100))\n",
    "print('Specificity: %.3f%%' % (true_neg / total_neg * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint['test_acc'][76])\n",
    "print(checkpoint['train_acc'][76])\n",
    "print(checkpoint['eval_acc'][76])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sens/spec of the Sigmoid model\n",
    "\n",
    "# Sensitivity: true positives / total positives\n",
    "# Specificity: true negatives / total negatives\n",
    "\n",
    "IMG_SIZE = (448, 448)\n",
    "\n",
    "test_dataset = XrayDataset('test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "model = YOLOSigmoidModel(IMG_SIZE)\n",
    "checkpoint = torch.load(BASE_DIR + '/torch_objects/modelYOLOSigmoid.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dicts'][9])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "true_neg = 0\n",
    "true_pos = 0\n",
    "total_pos = 0\n",
    "total_neg = 0\n",
    "for i_batch, (batch_X, batch_y) in enumerate(test_dataloader):\n",
    "  yhat = torch.round(torch.sigmoid(model.forward(batch_X)))\n",
    "  total += yhat.size(0)\n",
    "\n",
    "  batch_y = torch.flatten(batch_y)\n",
    "  y_compare = torch.flatten(yhat) == batch_y\n",
    "\n",
    "  # Very inefficient calculation but we don't really care about that b/c these\n",
    "  # calculations are incredibly fast compared to the speed of model.forward(...)\n",
    "  correct += torch.sum(y_compare)\n",
    "  true_pos += torch.sum(torch.logical_and(y_compare, batch_y == 1))\n",
    "  total_pos += torch.sum(batch_y == 1)\n",
    "  true_neg += torch.sum(torch.logical_and(y_compare, batch_y == 0))\n",
    "  total_neg += torch.sum(batch_y == 0)\n",
    "\n",
    "print('\\nOverall test accuracy for model[%d]: %.3f%%' % (26, correct / total * 100))\n",
    "print('Sensitivity: %.3f%%' % (true_pos / total_pos * 100))\n",
    "print('Specificity: %.3f%%' % (true_neg / total_neg * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sens/spec of the not all ReLU model\n",
    "\n",
    "# Sensitivity: true positives / total positives\n",
    "# Specificity: true negatives / total negatives\n",
    "\n",
    "class YOLOModelNotAll(torch.nn.Module):\n",
    "  def __init__(self, img_size):\n",
    "    super(YOLOModelNotAll, self).__init__()\n",
    "\n",
    "    # Required 448x448 image size\n",
    "    if img_size != (448, 448):\n",
    "      raise ValueError(\"Image size must be 448x448 to use the YOLO model.\")\n",
    "    \n",
    "    self.model_ID = 'YOLO'\n",
    "\n",
    "    # The network architecture from \"You Only Look Once: Unified, Real-Time Object Detection\"\n",
    "    # https://arxiv.org/abs/1506.02640\n",
    "    self.network = nn.Sequential(\n",
    "        nn.BatchNorm2d(3),\n",
    "        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(4, 4), stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.Conv2d(in_channels=32, out_channels=128, kernel_size=(2, 2), stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        \n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(1, 1), stride=1),\n",
    "        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(2, 2), stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(1, 1), stride=1),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(2, 2), stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        \n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(1, 1), stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(2, 2), stride=2, padding=1),\n",
    "        nn.Conv2d(in_channels=512, out_channels=256, kernel_size=(1, 1), stride=1),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(2, 2), stride=2),\n",
    "        nn.Conv2d(in_channels=256, out_channels=128, kernel_size=(1, 1), stride=1),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Flatten(),\n",
    "        nn.BatchNorm1d(2048),\n",
    "        \n",
    "        nn.Linear(2048, 512),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Linear(512, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.network(x)\n",
    "    return logits\n",
    "\n",
    "IMG_SIZE = (448, 448)\n",
    "\n",
    "test_dataset = XrayDataset('test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "model = YOLOModelNotAll(IMG_SIZE)\n",
    "checkpoint = torch.load(BASE_DIR + '/torch_objects/modelYOLO_first100.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dicts'][26])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "true_neg = 0\n",
    "true_pos = 0\n",
    "total_pos = 0\n",
    "total_neg = 0\n",
    "for i_batch, (batch_X, batch_y) in enumerate(test_dataloader):\n",
    "  yhat = torch.round(torch.sigmoid(model.forward(batch_X)))\n",
    "  total += yhat.size(0)\n",
    "\n",
    "  batch_y = torch.flatten(batch_y)\n",
    "  y_compare = torch.flatten(yhat) == batch_y\n",
    "\n",
    "  # Very inefficient calculation but we don't really care about that b/c these\n",
    "  # calculations are incredibly fast compared to the speed of model.forward(...)\n",
    "  correct += torch.sum(y_compare)\n",
    "  true_pos += torch.sum(torch.logical_and(y_compare, batch_y == 1))\n",
    "  total_pos += torch.sum(batch_y == 1)\n",
    "  true_neg += torch.sum(torch.logical_and(y_compare, batch_y == 0))\n",
    "  total_neg += torch.sum(batch_y == 0)\n",
    "\n",
    "print('\\nOverall test accuracy for model[%d]: %.3f%%' % (26, correct / total * 100))\n",
    "print('Sensitivity: %.3f%%' % (true_pos / total_pos * 100))\n",
    "print('Specificity: %.3f%%' % (true_neg / total_neg * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sens/spec of the sigmoid model\n",
    "\n",
    "# Sensitivity: true positives / total positives\n",
    "# Specificity: true negatives / total negatives\n",
    "\n",
    "class YOLOModelNotAll(torch.nn.Module):\n",
    "  def __init__(self, img_size):\n",
    "    super(YOLOModelNotAll, self).__init__()\n",
    "\n",
    "    # Required 448x448 image size\n",
    "    if img_size != (448, 448):\n",
    "      raise ValueError(\"Image size must be 448x448 to use the YOLO model.\")\n",
    "    \n",
    "    self.model_ID = 'YOLO'\n",
    "\n",
    "    # The network architecture from \"You Only Look Once: Unified, Real-Time Object Detection\"\n",
    "    # https://arxiv.org/abs/1506.02640\n",
    "    self.network = nn.Sequential(\n",
    "        nn.BatchNorm2d(3),\n",
    "        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(4, 4), stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.Conv2d(in_channels=32, out_channels=128, kernel_size=(2, 2), stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        \n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(1, 1), stride=1),\n",
    "        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(2, 2), stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(1, 1), stride=1),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(2, 2), stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        \n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(1, 1), stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(2, 2), stride=2, padding=1),\n",
    "        nn.Conv2d(in_channels=512, out_channels=256, kernel_size=(1, 1), stride=1),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(2, 2), stride=2),\n",
    "        nn.Conv2d(in_channels=256, out_channels=128, kernel_size=(1, 1), stride=1),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Flatten(),\n",
    "        nn.BatchNorm1d(2048),\n",
    "        \n",
    "        nn.Linear(2048, 512),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Linear(512, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.network(x)\n",
    "    return logits\n",
    "\n",
    "IMG_SIZE = (448, 448)\n",
    "\n",
    "test_dataset = XrayDataset('test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "model = YOLOModelNotAll(IMG_SIZE)\n",
    "checkpoint = torch.load(BASE_DIR + '/torch_objects/modelYOLO_first100.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dicts'][26])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "true_neg = 0\n",
    "true_pos = 0\n",
    "total_pos = 0\n",
    "total_neg = 0\n",
    "for i_batch, (batch_X, batch_y) in enumerate(test_dataloader):\n",
    "  yhat = torch.round(torch.sigmoid(model.forward(batch_X)))\n",
    "  total += yhat.size(0)\n",
    "\n",
    "  batch_y = torch.flatten(batch_y)\n",
    "  y_compare = torch.flatten(yhat) == batch_y\n",
    "\n",
    "  # Very inefficient calculation but we don't really care about that b/c these\n",
    "  # calculations are incredibly fast compared to the speed of model.forward(...)\n",
    "  correct += torch.sum(y_compare)\n",
    "  true_pos += torch.sum(torch.logical_and(y_compare, batch_y == 1))\n",
    "  total_pos += torch.sum(batch_y == 1)\n",
    "  true_neg += torch.sum(torch.logical_and(y_compare, batch_y == 0))\n",
    "  total_neg += torch.sum(batch_y == 0)\n",
    "\n",
    "print('\\nOverall test accuracy for model[%d]: %.3f%%' % (26, correct / total * 100))\n",
    "print('Sensitivity: %.3f%%' % (true_pos / total_pos * 100))\n",
    "print('Specificity: %.3f%%' % (true_neg / total_neg * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDhV4G6tnTOn"
   },
   "outputs": [],
   "source": [
    "# Our model has high sensitivity but low specificity, so let's train again starting from our best epoch,\n",
    "# but this time we'll use weighted sampling w/ pneumonia positive samples having double the weight of negatives\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "model = YOLOModel(IMG_SIZE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "\n",
    "train_dataset = XrayDataset('train')\n",
    "# Generate sample weights\n",
    "sample_weights = torch.tensor(train_dataset.y, dtype=float)\n",
    "sample_weights[sample_weights == 0] = 5.\n",
    "\n",
    "weighted_sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    weights=sample_weights, # quick way of generating weights that sum to 1 and are double for pneumonia samples\n",
    "    num_samples=len(train_dataset.y),\n",
    "    replacement=True\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, sampler=weighted_sampler)\n",
    "\n",
    "val_dataset = XrayDataset('val')\n",
    "test_dataset = XrayDataset('test')\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "train_binary_model(model, True, 100, train_dataloader, val_dataloader, test_dataloader, optimizer, model.model_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset = XrayDataset('train')\n",
    "# Generate sample weights\n",
    "sample_weights = torch.tensor(train_dataset.y, dtype=float)\n",
    "sample_weights[sample_weights == 0] = 5.\n",
    "\n",
    "weighted_sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    weights=sample_weights, # quick way of generating weights that sum to 1 and are double for pneumonia samples\n",
    "    num_samples=len(train_dataset.y),\n",
    "    replacement=True\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, num_workers=0, sampler=weighted_sampler)\n",
    "\n",
    "print(torch.sum(train_dataset.y == 0))\n",
    "print(torch.sum(train_dataset.y == 1))\n",
    "\n",
    "total_one = 0\n",
    "total_zero = 0\n",
    "for _, batch_y in enumerate(train_dataloader):\n",
    "    total_zero += torch.sum(batch_y == 0)\n",
    "    total_one += torch.sum(batch_y == 1)\n",
    "    \n",
    "    print(torch.flatten(batch_y))\n",
    "    break\n",
    "    \n",
    "print('Prop 0: %.2f' % (total_zero / (total_one+total_zero)))\n",
    "print('Prop 1: %.2f' % (total_one / (total_one+total_zero)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This block is for including and combining the full YOLO model w/ the last 50 epochs that Roshan trained\n",
    "\n",
    "# Sens/spec of the all ReLU model\n",
    "\n",
    "# Sensitivity: true positives / total positives\n",
    "# Specificity: true negatives / total negatives\n",
    "\n",
    "IMG_SIZE = (448, 448)\n",
    "\n",
    "test_dataset = XrayDataset('test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "model = YOLOModel(IMG_SIZE)\n",
    "checkpoint = torch.load(BASE_DIR + '/torch_objects/modelYOLOmoreReLuLast50Epoch.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dicts'][72])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "true_neg = 0\n",
    "true_pos = 0\n",
    "total_pos = 0\n",
    "total_neg = 0\n",
    "for i_batch, (batch_X, batch_y) in enumerate(test_dataloader):\n",
    "  yhat = torch.round(torch.sigmoid(model.forward(batch_X)))\n",
    "  total += yhat.size(0)\n",
    "\n",
    "  batch_y = torch.flatten(batch_y)\n",
    "  y_compare = torch.flatten(yhat) == batch_y\n",
    "\n",
    "  # Very inefficient calculation but we don't really care about that b/c these\n",
    "  # calculations are incredibly fast compared to the speed of model.forward(...)\n",
    "  correct += torch.sum(y_compare)\n",
    "  true_pos += torch.sum(torch.logical_and(y_compare, batch_y == 1))\n",
    "  total_pos += torch.sum(batch_y == 1)\n",
    "  true_neg += torch.sum(torch.logical_and(y_compare, batch_y == 0))\n",
    "  total_neg += torch.sum(batch_y == 0)\n",
    "\n",
    "print('\\nOverall test accuracy for model[%d]: %.3f%%' % (26, correct / total * 100))\n",
    "print('Sensitivity: %.3f%%' % (true_pos / total_pos * 100))\n",
    "print('Specificity: %.3f%%' % (true_neg / total_neg * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ##############################################################################\n",
    "# THIS CODE BLOCK IS SPECIFICALLY FOR GENERATING THE COMBINED YOLOMODEL ACC/LOSS PLOT #\n",
    "    ##############################################################################\n",
    "\n",
    "# Testing the performance of the the best performing iteration of model 7\n",
    "model = YOLOSigmoidModel(IMG_SIZE)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "checkpoint = torch.load(BASE_DIR + '/torch_objects/modelYOLOmoreReLuLast50Epoch.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = XrayDataset('train')\n",
    "val_dataset = XrayDataset('val')\n",
    "test_dataset = XrayDataset('test')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(checkpoint['train_acc'])), checkpoint['train_acc'], label=\"Training\")\n",
    "plt.plot(range(len(checkpoint['eval_acc'])), checkpoint['eval_acc'], label=\"Validation\")\n",
    "plt.plot(range(len(checkpoint['test_acc'])), checkpoint['test_acc'], label=\"Testing\")\n",
    "\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.axvline(x=100, linestyle='--', color='black')\n",
    "plt.text(103, 0.65, 'Epoch=100')\n",
    "\n",
    "plt.savefig(\"YOLOModel_yolo150_acc.png\", dpi=1000)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "adj_costs = torch.stack([torch.tensor(checkpoint['train_cost']) / len(train_dataloader), torch.tensor(checkpoint['eval_cost']) / len(val_dataloader), torch.tensor(checkpoint['test_cost']) / len(test_dataloader)])\n",
    "# adj_costs[range(0, 3, 2), 17:] /= 2\n",
    "length = len(checkpoint['train_cost'])\n",
    "\n",
    "plt.plot(range(length), adj_costs[0], label=\"Training\")\n",
    "plt.plot(range(length), adj_costs[1], label=\"Validation\")\n",
    "plt.plot(range(length), adj_costs[2], label=\"Testing\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.axvline(x=100, linestyle='--', color='black')\n",
    "plt.text(103, 0.65, 'Epoch=100')\n",
    "\n",
    "plt.savefig(\"YOLOModel_yolo150_loss.png\", dpi=1000)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3kxBRWj5bbEk",
    "12U50KYbZfJJ",
    "JyYkDZgyNqI_",
    "Qq2He0A88kxK",
    "eVW1qzZHYVyg"
   ],
   "name": "Pnotebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
